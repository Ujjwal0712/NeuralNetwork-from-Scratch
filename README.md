This project implements a basic neural network from scratch using only Numpy. The aim is to understand the underlying mechanics of neural networks by building and training a model for a simple classification task. 
The network includes essential components such as forward propagation, backpropagation, and gradient descent for learning.

Features

 1. Fully Connected Neural Network: Implements a multi-layer perceptron with customizable architecture.
 2. Numpy-Based Implementation: No deep learning frameworks are used; the project relies solely on Numpy for all computations.
 3. Forward and Backward Propagation: Implements both the forward pass to compute predictions and the backward pass to update weights based on gradients.
 4. Gradient Descent: Uses gradient descent optimization to minimize the loss function.
 5. Activation Functions: Supports sigmoid, ReLU, and softmax activation functions.
 6. Loss Functions: Implements mean squared error (MSE) and cross-entropy loss functions.
